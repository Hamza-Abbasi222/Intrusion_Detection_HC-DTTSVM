# -*- coding: utf-8 -*-
"""HC-DTTWSVM on UNSW NB 15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ndz6E2c3G-gWaeSpwd3m2qEzohVjKMQr

# **Data PreProcessing**
"""

# Commented out IPython magic to ensure Python compatibility.
# data cleaning and plots
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('ggplot')
# %matplotlib inline

# sklearn: data preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder

# sklearn: train model
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold
from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report

# sklearn classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler


# Load the dataset
test_df = pd.read_csv('/content/drive/Shareddrives/CyberSecurity Dataset/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_test-set.csv')
train_df = pd.read_csv('/content/drive/Shareddrives/CyberSecurity Dataset/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_training-set.csv')
OD = pd.concat([test_df, train_df], ignore_index=True)
initial_data = OD
df = initial_data
#train_df = pd.read_csv('UNSW_NB15_training-set.csv')
#test_df = pd.read_csv('UNSW_NB15_testing-set.csv')

#Drop some files which were not included in code file
df = df.drop(axis=1, columns=['id' , 'proto' , 'service' ,'state'])

df

df['attack_cat'].unique()

import pandas as pd
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2


# Split the data into features (X) and target (y)
X = df.drop(columns=['attack_cat', 'label'], axis=1)
y = df['attack_cat']

# Initialize the feature selector with the desired scoring function and k value
selector = SelectKBest(score_func=chi2, k=10)

# Fit the selector to the data to obtain the selected features
selector.fit(X, y)
selected_features = selector.get_support(indices=True)

# Subset the data to keep only the selected features
X_new = X.iloc[:, selected_features]

# Define the feature and target columns
feature_cols = X_new.columns
target_col = 'attack_cat'

'''
# Define the feature and target columns
feature_cols = df.columns[:-2]
target_col = 'attack_cat'
'''

#######################
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[feature_cols], df[target_col], test_size=0.3, random_state=57)

"""# Origional HC-DTTWSVM"""

print('X_train.shape is ', X_train.shape)
print('X_test.shape is ', X_test.shape)
print('Y_Train.shape is ', y_train.shape)
print('Y_test.shape is ', y_test.shape)
print('Origional Data.shape is ', OD.shape)
print('after drop string columns.shape is ',df.shape)

'''
###############
feature_to_encode = data_UNSW['attack_cat']
encoder = LabelEncoder()
encoder.fit(feature_to_encode)
encoded_feature = encoder.transform(feature_to_encode)
data_UNSW['attack_cat'] = encoded_feature
'''

###
# Preprocess the data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(train_df[feature_cols].values)
y_train = train_df[target_col].values
X_test = scaler.transform(test_df[feature_cols].values)
y_test = test_df[target_col].values

# Define the hierarchical structure
hierarchy = {
    'Normal': ['Normal'],
    'Attack': ['Backdoor', 'Analysis', 'Fuzzers', 'Shellcode', 'Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic'],
    'DoS': ['DoS'],
    'Probe': ['Reconnaissance'],
    'U2R': ['Backdoor', 'Shellcode'],
    'R2L': ['Worms', 'Generic', 'Exploits', 'Fuzzers', 'Analysis']
}

# Train the HC-DTTWSVM model
models = {}
for node, children in hierarchy.items():
    if node == 'Normal':
        continue
    y_train_node = train_df[target_col].isin(children)
    model = svm.OneClassSVM(kernel='rbf', gamma='scale')
    model.fit(X_train, y_train_node)
    models[node] = model

# Test the HC-DTTWSVM model
y_pred = []
for i, instance in test_df.iterrows():
    node = 'Attack'
    while node != 'Normal':
        model = models[node]
        y_pred_node = model.predict(X_test[i].reshape(1, -1))
        if y_pred_node == 1:
            node = 'Normal'
        else:
            for child_node, children in hierarchy.items():
                if node in children:
                    node = child_node
                    break
    y_pred.append(node)

# Evaluate the performance of the HC-DTTWSVM model
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

"""# **GRU_XGBoost Binary Classification**"""

##########################################
#######   Binary Classification ##########
##########################################

# Load the UNSWNB15 dataset
df = initial_data
#df = df1

#Drop some files which were not included in code file
df = df.drop(axis=1, columns=['id' , 'proto' , 'service' ,'state'])

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, GRU, Dropout
from xgboost import XGBClassifier

# Select top 10 features using SelectKBest and chi2
X = df.iloc[:, :-2]
y = df.iloc[:, -1]
selector = SelectKBest(chi2, k=10)
X = selector.fit_transform(X, y)
feature_indices = selector.get_support(indices=True)

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Get the number of time steps and number of features in each packet
num_time_steps = 10
num_features = X_train.shape[1]

# Reshape the input sequences to 3D arrays
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))

# Build the GRU model
model_gru = Sequential()
model_gru.add(GRU(units=128, input_shape=(num_time_steps, 1)))
model_gru.add(Dropout(0.2))
model_gru.add(Dense(units=10, activation='softmax'))

# Compile the GRU model
model_gru.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the GRU model
model_gru.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Get the predictions of the GRU model on the validation set
y_val_pred_gru = np.argmax(model_gru.predict(X_val), axis=-1)

# Build the XGBoost model
model_xgb = XGBClassifier()

# Train the XGBoost model
model_xgb.fit(X_train.reshape((X_train.shape[0], X_train.shape[1] * X_train.shape[2])), y_train)

# Get the predictions of the XGBoost model on the validation set
y_val_pred_xgb = model_xgb.predict(X_val.reshape((X_val.shape[0], X_val.shape[1] * X_val.shape[2])))

# Print the accuracy of the GRU and XGBoost models on the validation set
print('GRU accuracy on validation set: ', np.mean(y_val_pred_gru == y_val))
print('XGBoost accuracy on validation set: ', np.mean(y_val_pred_xgb == y_val))

from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report

# calculate the confusion matrix
cm = confusion_matrix(y_val, y_val_pred_gru)

#cm_ontest = confusion_matrix(y_true=y1_test_transform, y_pred=y_pred_class)
#print('Confusion Matrix GRU:\n', cm_ontest)
print('Confusion Matrix GRU:\n', cm)

# Define the class labels for your data
class_labels = ['Normal', 'attack']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for GRU')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()

from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report

# calculate the confusion matrix
cm = confusion_matrix(y_val, y_val_pred_xgb)

#cm_ontest = confusion_matrix(y_true=y1_test_transform, y_pred=y_pred_class)
print('Confusion Matrix XGBoost:\n', cm)

# Define the class labels for your data
class_labels = ['Normal', 'Attack']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for XGBoost')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()

"""# **GRU_XGBoost Multiclass classification**"""

##########################################
##### Attack Type Classification #########
##########################################

# Load the UNSWNB15 dataset
df = initial_data
#df = df1

#Drop some files which were not included in code file
df = df.drop(axis=1, columns=['id' , 'proto' , 'service' ,'state'])

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, GRU, Dropout
from xgboost import XGBClassifier

from sklearn.preprocessing import LabelEncoder
# Select the column you want to encode
attack_cat = df['attack_cat']

# Create a LabelEncoder object
le = LabelEncoder()

# Fit and transform the attack_cat column to integer labels
df['attack_cat'] = le.fit_transform(attack_cat)

# Select top 10 features using SelectKBest and chi2
X = df.iloc[:, :-2]
y = df.iloc[:, -2]
selector = SelectKBest(chi2, k=20)
X = selector.fit_transform(X, y)
feature_indices = selector.get_support(indices=True)

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Get the number of time steps and number of features in each packet
num_time_steps = 20
num_features = X_train.shape[1]

# Reshape the input sequences to 3D arrays
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))

# Build the GRU model
model_gru = Sequential()
model_gru.add(GRU(units=128, input_shape=(num_time_steps, 1)))
model_gru.add(Dropout(0.2))
model_gru.add(Dense(units=10, activation='softmax'))

# Compile the GRU model
model_gru.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the GRU model
model_gru.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

import pickle

# Assume that 'model' is your trained CNN model
with open('/content/drive/MyDrive/ML on Cyber Security Dataset/Working March 2023/Models/UNSW NB 15 Dataset/model_gru.pkl', 'wb') as f:
    pickle.dump(model_gru, f)

# Get the predictions of the GRU model on the validation set
y_val_pred_gru = np.argmax(model_gru.predict(X_val), axis=-1)

# Build the XGBoost model
model_xgb = XGBClassifier()

# Train the XGBoost model
model_xgb.fit(X_train.reshape((X_train.shape[0], X_train.shape[1] * X_train.shape[2])), y_train)

# Get the predictions of the XGBoost model on the validation set
y_val_pred_xgb = model_xgb.predict(X_val.reshape((X_val.shape[0], X_val.shape[1] * X_val.shape[2])))

# Print the accuracy of the GRU and XGBoost models on the validation set
print('GRU accuracy on validation set: ', np.mean(y_val_pred_gru == y_val))
print('XGBoost accuracy on validation set: ', np.mean(y_val_pred_xgb == y_val))

from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report

# calculate the confusion matrix
cm = confusion_matrix(y_val, y_val_pred_gru)

#cm_ontest = confusion_matrix(y_true=y1_test_transform, y_pred=y_pred_class)
print('Confusion Matrix GRU:\n', cm)

# Define the class labels for your data
class_labels = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for GRU')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()

from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report

# calculate the confusion matrix
cm = confusion_matrix(y_val, y_val_pred_xgb)

#cm_ontest = confusion_matrix(y_true=y1_test_transform, y_pred=y_pred_class)
print('Confusion Matrix XGBoost:\n', cm)

# Define the class labels for your data
class_labels = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for XGBoost')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()

import pickle

# Assume that 'model' is your trained CNN model
with open('/content/drive/MyDrive/ML on Cyber Security Dataset/Working March 2023/Models/UNSW NB 15 Dataset/model_xgb.pkl', 'wb') as f:
    pickle.dump(model_xgb, f)

"""# **LSTM XGBoost New**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import xgboost as xgb
from sklearn.metrics import accuracy_score

# Load the data
data = train_df
#Drop some files which were not included in code file
data = data.drop(axis=1, columns=['id' , 'proto' , 'service' ,'state'])

from sklearn.preprocessing import LabelEncoder
# Select the column you want to encode
attack_cat = data['attack_cat']

# Create a LabelEncoder object
le = LabelEncoder()

# Fit and transform the attack_cat column to integer labels
data['attack_cat'] = le.fit_transform(attack_cat)

# Preprocess the data
le = LabelEncoder()
for col in data.columns:
    if data[col].dtype == 'object':
        data[col] = le.fit_transform(data[col])
data.fillna(0, inplace=True)
scaler = StandardScaler()
X = data.iloc[:, :-2].values
X = scaler.fit_transform(X)
y = data.iloc[:, -2].values

# Feature Selection
skb = SelectKBest(f_classif, k=20)
X = skb.fit_transform(X, y)

# LSTM Model
X_lstm = np.reshape(X, (X.shape[0], 1, X.shape[1]))
model_lstm = Sequential()
model_lstm.add(LSTM(units=64, input_shape=(1, X.shape[1]), activation='relu'))
model_lstm.add(Dense(units=1, activation='sigmoid'))
model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_lstm.fit(X_lstm, y, epochs=20, batch_size=64)

# XGBoost Model
model_xgb = xgb.XGBClassifier()
model_xgb.fit(X, y)

from sklearn.metrics import classification_report
import xgboost as xgb

# Assuming you have trained and saved your XGBoost model as `model_xgb`
# Assuming you have the test data as `X_test` and `y_test`

# Generate predictions from the XGBoost model
y_pred = model_xgb.predict(X_val)

# Convert the predicted probabilities to class labels (if needed)
# y_pred = np.argmax(y_pred, axis=1)

# Compute the classification report
report = classification_report(y_val, y_pred)

# Print the classification report
print(report)

# Ensemble Model
y_pred_lstm = model_lstm.predict(X_lstm)
y_pred_xgb = model_xgb.predict(X)
y_pred_ensemble = (y_pred_lstm.ravel() + y_pred_xgb.ravel()) / 2
y_pred_ensemble[y_pred_ensemble >= 0.5] = 1
y_pred_ensemble[y_pred_ensemble < 0.5] = 0

# Evaluation
accuracy = accuracy_score(y, y_pred_lstm)
print('LSTM Model Accuracy:', accuracy)

accuracy = accuracy_score(y, y_pred_xgb)
print('XGB Model Accuracy:', accuracy)

accuracy = accuracy_score(y, y_pred_ensemble)
print('Ensemble Model Accuracy:', accuracy)

from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report

# calculate the confusion matrix
cm = confusion_matrix(y, y_pred_ensemble)


#cm_ontest = confusion_matrix(y_true=y1_test_transform, y_pred=y_pred_class)
print('Confusion Matrix Ensamble:\n', cm)

# Define the class labels for your data
class_labels = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for Ensamble')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()

accuracy = accuracy_score(y, y_pred_ensemble)
print('Ensemble Model Accuracy:', accuracy)

from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report

# calculate the confusion matrix
cm = confusion_matrix(y, y_pred_xgb)

#cm_ontest = confusion_matrix(y_true=y1_test_transform, y_pred=y_pred_class)
print('Confusion Matrix XGBoost:\n', cm)

# Define the class labels for your data
class_labels = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for XGBoost')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()
accuracy = accuracy_score(y, y_pred_xgb)
print('XGB Model Accuracy:', accuracy)

from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report

# calculate the confusion matrix
cm = confusion_matrix(y, y_pred_lstm)

#cm_ontest = confusion_matrix(y_true=y1_test_transform, y_pred=y_pred_class)
print('Confusion Matrix LSTM:\n', cm)

# Define the class labels for your data
class_labels = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for LSTM')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()
accuracy = accuracy_score(y, y_pred_lstm)
print('LSTM Model Accuracy:', accuracy)

"""

```
# trying to implement DNN and CNN whlie getting an error in upper portion
```

# NEW DNN and CNN"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from google.colab import drive
drive.mount('/content/drive')

# Load data from CSV file
test_df = pd.read_csv('/content/drive/Shareddrives/CyberSecurity Dataset/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_test-set.csv')
train_df = pd.read_csv('/content/drive/Shareddrives/CyberSecurity Dataset/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_training-set.csv')
data = pd.concat([test_df, train_df], ignore_index=True)
# data = df

# Data cleaning
data = data.dropna()

# Feature engineering
encoder = LabelEncoder()
data['attack_cat'] = encoder.fit_transform(data['attack_cat'])
data.drop(columns=['id', 'proto', 'service', 'state'], inplace=True)
X = data.iloc[:, :-2].values
y = data.iloc[:, -2].values

# Feature engineering
X = data.drop(['attack_cat'], axis=1)
y = data['attack_cat']

# Feature scaling
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Feature selection
selector = SelectKBest(score_func=f_classif, k=20)
X = selector.fit_transform(X, y)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build DNN model
dnn_model = Sequential()
dnn_model.add(Dense(units=64, activation='relu', input_dim=20))
dnn_model.add(Dense(units=32, activation='relu'))
dnn_model.add(Dense(units=10, activation='softmax'))
dnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

# this is because of the binary or multicalss classification
#dnn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


# Train DNN model
early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)
dnn_model.fit(X_train, pd.get_dummies(y_train), validation_data=(X_test, pd.get_dummies(y_test)), epochs=50, batch_size=32, callbacks=[early_stop])

# Build CNN model
cnn_model = Sequential()
cnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(20, 1)))
cnn_model.add(MaxPooling1D(pool_size=2))
cnn_model.add(Flatten())
cnn_model.add(Dense(units=64, activation='relu'))
cnn_model.add(Dense(units=10, activation='softmax'))
cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

# Reshape data for CNN model
X_train_cnn = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test_cnn = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# Train CNN model
cnn_model.fit(X_train_cnn, pd.get_dummies(y_train), validation_data=(X_test_cnn, pd.get_dummies(y_test)), epochs=50, batch_size=32, callbacks=[early_stop])

# Print model summary
print("CNN Model Summary:")
print(cnn_model.summary())
print("\nDNN Model Summary:")
print(dnn_model.summary())

from keras.layers import Reshape

# Ensemble model
ensemble_model = Sequential()
ensemble_model.add(Dense(units=64, activation='relu', input_dim=20))
ensemble_model.add(Reshape((64, 1)))  # Reshape layer
ensemble_model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
ensemble_model.add(MaxPooling1D(pool_size=2))
ensemble_model.add(Flatten())
ensemble_model.add(Dense(units=64, activation='relu'))
ensemble_model.add(Dense(units=10, activation='softmax'))
ensemble_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

#Getting an error in this setion so add new section Before
'''
# Ensemble model
ensemble_model = Sequential()
ensemble_model.add(Dense(units=64, activation='relu', input_dim=20))
ensemble_model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(20, 1)))
ensemble_model.add(MaxPooling1D(pool_size=2))
ensemble_model.add(Flatten())
ensemble_model.add(Dense(units=64, activation='relu'))
ensemble_model.add(Dense(units=10, activation='softmax'))
ensemble_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])
'''

from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, concatenate
from tensorflow.keras.models import Model

input1 = Input(shape=(20,))
dnn_layer1 = Dense(units=64, activation='relu')(input1)
dnn_layer2 = Dense(units=32, activation='relu')(dnn_layer1)

input2 = Input(shape=(20, 1))
cnn_layer1 = Conv1D(filters=32, kernel_size=3, activation='relu')(input2)
cnn_layer2 = MaxPooling1D(pool_size=2)(cnn_layer1)
cnn_layer3 = Flatten()(cnn_layer2)

merged = concatenate([dnn_layer2, cnn_layer3])
ensemble_output = Dense(units=10, activation='softmax')(merged)

ensemble_model = Model(inputs=[input1, input2], outputs=ensemble_output)
ensemble_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

ensemble_model.fit([X_train, X_train_cnn], pd.get_dummies(y_train), validation_data=([X_test, X_test_cnn], pd.get_dummies(y_test)), epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model on test data
ensemble_loss, ensemble_acc = ensemble_model.evaluate([X_test, X_test_cnn], pd.get_dummies(y_test))

# Get predictions for test data
y_pred = ensemble_model.predict([X_test, X_test_cnn])

# Convert predictions to classes
y_pred_classes = np.argmax(y_pred, axis=1)

# Get true classes
y_true = y_test

# Calculate confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Print metrics
print("Test loss:", ensemble_loss)
print("Test accuracy:", ensemble_acc)
print("Confusion matrix:")
print(cm)

# Define the class labels for your data
class_labels = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for Ensemble DNN & CNN')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()

# Evaluate ensemble model
ensemble_loss, ensemble_acc = ensemble_model.evaluate([X_test, X_test_cnn], pd.get_dummies(y_test))
#ensemble_loss, ensemble_acc = ensemble_model.evaluate(X_test, y_test)
#print("Ensemble model - loss: {:.3f}, accuracy: {:.3f}".format(ensemble_loss, ensemble_acc))



# Evaluate the model on test data
y_pred = ensemble_model.predict([X_test, X_test_cnn])
#y_pred = ensemble_model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
print("Accuracy of the Ensemble Model: {:.2f}%".format(accuracy_score(y_test, y_pred) * 100))
print("Confusion Matrix of the Ensemble Model:")
print(confusion_matrix(y_test, y_pred))
print("Classification Report of the Ensemble Model:")
print(classification_report(y_test, y_pred))

# Define the class labels for your data
class_labels = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for XGBoost')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()

from sklearn.metrics import confusion_matrix

# Predict on test set using CNN model
y_pred_cnn = cnn_model.predict(X_test)
y_pred_cnn = np.argmax(y_pred_cnn, axis=1)

# Get confusion matrix for each model
cm_cnn = confusion_matrix(y_test, y_pred_cnn)

'''
# Evaluate DNN model
dnn_loss, dnn_accuracy = dnn_model.evaluate(X_test, y_test)
print("DNN Loss:", dnn_loss)
print("DNN Accuracy:", dnn_accuracy)
'''

print("CNN Confusion Matrix:")
print(cm_cnn)

# Predict on test set using DNN model
y_pred_dnn = dnn_model.predict(X_test)
y_pred_dnn = np.argmax(y_pred_dnn, axis=1)

cm_dnn = confusion_matrix(y_test, y_pred_dnn)

'''
# Evaluate CNN model
cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test)
print("CNN Loss:", cnn_loss)
print("CNN Accuracy:", cnn_accuracy)
'''

print("DNN Confusion Matrix:")
print(cm_dnn)

# Define the class labels for your data
class_labels = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm_cnn, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for CNN')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()




# Define the class labels for your data
class_labels = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
# Visualize the confusion matrix using a heatmap
sns.heatmap(cm_dnn, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion matrix for DNN')
plt.xlabel('Predicted class')
plt.ylabel('True class')
plt.show()

"""# New Section

# New Section
"""